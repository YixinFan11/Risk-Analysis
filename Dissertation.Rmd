---
title: "Dissertation"
author: "Yixin Fan.  Can No.22018352:"
date: "14/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("moments")
#install.packages("expm")
```

In this dissertation, I will use R Markdown to finish both code and analysis, and the version is R version 3.5.1 (2018-07-02).


Question 1: Generation of (pseudo-)random numbers

Consider Chi-square Distribution with degree of freedom k. It has finite mean of k and variance of 2k. In this question, the degree of freedom is set as $k=4$. Therefore, its pdf is: $$f(x;k)=f(x;4)=\frac{1}{2^{k/2}\Gamma(k/2)}x^{k/2-1}e^{-x/2}=\frac{1}{4}xe^{-x/2}$$
Its cdf is $$F(x;k)=\frac{1}{\Gamma(k/2)}\gamma(\frac{k}{2},\frac{x}{2})=\gamma(2,\frac{x}{2})=\int_{0}^{\frac{x}{2}} te^{-t} \,dt=1-\frac{x}{2}e^{-x/2}-e^{-x/2}$$
From its cdf and pdf, it can be observed that inversion methods will be complicated, since it is difficult to compute the inverse function of cdf. Hence, Rejection Method, which accept the point under the curve and reject the point above the curve, will be more suitable to use. The r code is as follow:

```{r}
N <- 30000 # number of iterations
X <- rep(NA,N) # initialize the condition
k <- 4 # degree of freedom
for (i in 1:N)
{
  Y <- 10*runif(1) # uniform deviates from 0 to 10
  P <- runif(1) # uniform deviates from 0 to 1
  if (P<=Y/4*exp(-Y/2)) 	
    X[i]=Y #Accept Y if the random selected variable P is smaller or equal to the f(Y)
}
index <- which(X!="NA")
X <- X[index] # remove the rejcted value/remove the NA in X

length(X)/N # acceptance rate

#This is the theoretical value of pdf of Chi-square distribution with degree 4
x<-seq(0,10,by=0.5)#Set value of x with each gap 0.5
y<-x/4*exp(-x/2)#Calculate the corresponding theoretical value

hist(X,breaks=30,probability=TRUE)#plot the histogram with 30 bars
lines(x,y)#superimposing the theoretical value as a line on the histogram
```

Goodness of fit test: Jarqueâ€“Bera test measures whether the data matches a normal distribution by the skewness and kurtosis. The test statistics JB is defined as: $$JB=\frac{n}{6}(S^2+\frac{1}{4}(K-3)^2)$$
where n is the number of the observation, S is the sample skewness, K is the sample kurtosis.

```{r}
lr <- diff(X) #X is the observations from the above code
#Calculate the mean and the standard deviation using the function mean() and sd()
mlr <- mean(lr)
stdlr <- sd(lr)
#install.packages("moments")
library(moments)
JBstat <- rep(NA,10000)
for (i in 1:10000){
lrth <- rnorm(length(lr),mean=mlr,sd=stdlr)
JBstat[i] <- length(lr)*(skewness(lrth)^2+(kurtosis(lrth)-3)^2/4)/6
}
hist(JBstat, breaks=50, probability=TRUE)

F10 <- ecdf(JBstat)
summary(F10)

plot(F10, verticals = TRUE, do.points = FALSE)
length(lr)*(skewness(lr)^2+(kurtosis(lr)-3)^2/4)/6
```


Question 2: MCMC

The Markov chain Monte Carlo consist of two parts: Monte Carlo and Markov Chain. As for Monte Carlo, it allows people to calculate means or other index from large sample rather than the distribution function(van Ravenzwaaij,2018). This is will make the calculation simpler to work and can make sure it can be worked out, since sometimes it is hard to find the distribution function and at meantime, sample is always easy to obtain. As for Markov Chain, MCMC uses Markov property to generate samples by a special process: The random samples will continue to generate new random samples(van Ravenzwaaij,2018). Specially, according to Markov Property, the new random sample, depend only on the present sample which generate it, but not depend on any other previous random sample. Therefore, the main reason that why it works is that MCMC goes from sample to distribution.

The following process of obtaining the final invariant distribution:

Step 1: set initial conditions randomly
Step 2: generate new random variables according to the present random variable and the transition matrix(in other words, restrictions)
Step 3: According to the transition, generate enough samples and then stop. The following r code generates $10^5$ samples.
Then, based on the samples that has generate, the sample will enable people to analyse.


I will set the matrix as follow: 
$$\begin{bmatrix} 
 \alpha & \beta & \gamma \\ 
 \delta & \epsilon & \zeta \\ 
 \eta & \theta & \iota 
 \end{bmatrix}
 = 
 \begin{bmatrix} 
 0.5 & 0.5 & 0 \\ 
 0.35 & 0.3 & 0.35 \\ 
 0 & 0.5 & 0.5 
 \end{bmatrix}$$
with all the row sums equals to 1. This matrix can be explained as: Given three states A, B and C, the probability of going from A, B or C to themselves are 0.5, 0.3, 0.5 respectively, which gives $p_{11}=0.5, p_{22}=0.3, p_{33}=0.5$. And the probability from A to B is $p_{12}=0.5$, from B to C is $p_{23}=0.35$, from B to A is $p_{21}=0.35$, from C to B is $p_{32}=0.5$. 

This Markov Chain is irreducible and aperiodic. The definition for aperiodic according to the transition matrix is: $p_{ii}^{(1)}>0$. And from the matrix, $p_{11}=0.5>0, p_{22}=0.3>0, p_{33}=0.5>0$, which means that this is aperiodic

Irreducible is defined: any state, which is in state S, can reach another state. In other words,$\exists n \in \mathbb{N}^{*}$ such that $p_{ij}^{(n)}>0$ $\forall i,j \in \{1,2,3\}$ . From the code below, it can be concluded that this Markov Chain is irreducible since $p_{ij}^{(2)}>0, \forall i,j \in \{1,2,3\}$
```{r}
library(expm)
PMatrix1 <- matrix(rep(0,9),nrow=3,ncol=3)

PMatrix1[1,1] <- 0.5
PMatrix1[2,2] <- 0.3
PMatrix1[3,3] <- 0.5

PMatrix1[1,2] <- 0.5
PMatrix1[2,1] <- 0.35
PMatrix1[2,3] <- 0.35
PMatrix1[3,2] <- 0.5

PMatrix1
PMatrix1 %^% 2
PMatrix1 %^% 200
```

The invariant distribution $\pi$ is defined as for every $i,j\in S$, the limit $\pi=\lim_{k\to\infty} p_{i,j}^{(k)}$ exists and is independent of i. It also has such property that if state S is finite, then $\sum_{j\in S}\pi_j=1$, and $\pi P=\pi$, where P is the P-matrix, and $\pi$is the eigenvectors for matrix P with eigenvalue 1. Therefore, the theoretical invariant distribution $\pi = (\pi_1, \pi_2, \pi_3)$ is calculated as follow:

$$(\pi_1, \pi_2, \pi_3)
\begin{bmatrix} 
 0.5 & 0.5 & 0 \\ 
 0.35 & 0.3 & 0.35 \\ 
 0 & 0.5 & 0.5 
 \end{bmatrix}
 =(\pi_1, \pi_2, \pi_3)\\$$
This gives the following four equations:

$$ 0.5\pi_1+ 0.35\pi_2 =\pi_1 $$
$$   0.5\pi_1+ 0.3\pi_2+0.5\pi_3 =\pi_2$$
$$   0.35\pi_2+ 0.5\pi_3 =\pi_3$$
$$   \pi_1+ \pi_2+ \pi_3 =1$$
Solve the equations and get:
$$(\pi_1,\pi_2,\pi_3)=(\frac{7}{24},\frac{5}{12},\frac{7}{24})\approx(0.292,0.417,0.292)$$
```{r}
# Consider 3 states: 0,1,2 in this simulations

freq <- rep(0,3) # initialize the frequency
# Set the initial conditions, it can be any kinds:(0,0), (0,1), (1,0) ,(1,1) and will not influence the final result
X1 <- rep(0,2) 

Niter <- 100000 # number of iterations
Kstates <- rep(0,Niter) # vector of states according to iterations

#According to my transition matrix, At state 0 and 2, the probability of remain unchanged is 0.5. At state 1, the probability of remain unchanged is 0.3.
for (i in 1:Niter) {
# select one of the position from X1 and change the number on its position. In other words, change 1 to 0 or change 0 to 1.
	k1 <- sample(2, size = 1)
#Two conditions for different remaining probabilities.
	if (sum(X1)==1)
	  if (runif(1)<0.7){X1[k1] <- -X1[k1]+1} else {X1[k1] <-X1[k1]}
	else
	  if (runif(1)<1/2){X1[k1] <- -X1[k1]+1} else {X1[k1] <-X1[k1]}
# sum(X1) is the states of each iteration(after each moving) and put it into the corresponding position in Kstates, forming the state against each time of action
	Kstates[i] <- sum(X1)
	}
	
# Compute the frequency of each state(0, 1, 2) appeared in the vector Kstates.
for (j in 1:3) {
freq[j] <- length(which(Kstates==3-j))/Niter
}

# Comparisons: Frequency vs equilibrium probability

# Theoretical equilibrium distribution
pi <- c(7/24,5/12,7/24)
freq
pi
state = c(1:3)

plot(state,freq,xlab="state",ylab="Pr(state)")
lines(state,pi,type="h")
```
From the diagram and the calculation above, it can be conclude that as iteration gets larger, the empirical distribution will get more close to the theoretical invariant distribution. And as iteration goes to infinity, the empirical distribution will equal to the theoretical invariant distribution.




Question 3: Random Walk

In Question 1, Chi-Squared Distribution is chosen. Consider the new distribution $Z_N=\sum_{i=1}^{N}X_i$, where $X_i\sim \chi^2(4)$. Compute $Z_N$ using the characteristic function. The characteristic function of $\chi^2(4)$ is $$(1-2it)^{-k/2}$$
Therefore, $${\hat f}_{Z_N}=\prod_{i=1}^{N}{\hat f}_{X_i}=[(1-2it)^{-k/2}]^N=(1-2it)^{-kN/2}$$
Hence, ${Z_N}\sim \chi^2(4N)$

```{r}
#Number of iterations
n3 <- 10000 
#Number of N, meaning N Chi-Square Distribution sum up
N3 <- 1000
#Degree of freedom, same as the first question
k <- 4

# Initialize the condition
Chi1 <- rep(NA,n3)

# The idea of this loop is to generate the random numbers from 0 to infinity with probability under the Chi-Squared Distribution. Therefore, the first step is to randomly select a probability and then get the corresponding random number using the function 'qchisq'. Finally sum up all the numbers(N3 in total) and finish this single loop.
for (i in 1:n3) {
  prob3 <- runif(N3)
  x3 <- qchisq(prob3, df=4,lower.tail = TRUE)
  Chi1[i] <- sum(x3)
} 

#Histogram of the summation with 50 bars
hist(Chi1,50,probability='TRUE')

#Normalize the result using the fomula (Z-N*mean)/sqrt(N*Var)
Normal1 = (Chi1-N3*k)/sqrt(N3*k*2)
hist(Normal1,50,probability='TRUE')
#Create an interval from -4 to 4 by 0.1 gap. The interval can be any length but better to be symmetric. Finally, superimposing the Standard Normal Distribution curve, which is N(0,1) curve on the histogram.
x4 <- seq(-4, 4, 0.1)
lines(x4, dnorm(x4, 0, 1))

library(tseries)
jarque.bera.test(Normal1)
```
From the result above, it can be concluded that as N goes to infinity, the distribution tends to a normal distribution and after normalization it will follow a standard normal distribution with mean equals to 0 and Variance equal to 1. The result of p value is very small which means that if we set a confidence interval of 5%, we can easy to draw the conclusion that the null hypothesis (it follows Normal Distribution) will be accepted. 

In theoretical Calculation, ${Z_N}\sim \chi^2(kN)$. If set $K=kN$, as N goes to infinity, K also goes to infinity. Therefore, the conclusion is for a Chi-squared Distribution with degree of freedom s, $$\lim_{s\rightarrow \infty}\chi^2(s)\sim N(s,2s)$$
And $$\lim_{s\rightarrow \infty}\frac{\chi^2-s}{\sqrt{2s}}\sim N(0,1)$$

References:
[1]van Ravenzwaaij, D., Cassey, P. & Brown, S.D. A simple introduction to Markov Chain Monteâ€“Carlo sampling. Psychon Bull Rev 25, 143â€“154 (2018).Available at: https://doi.org/10.3758/s13423-016-1015-8

